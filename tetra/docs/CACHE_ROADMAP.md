# ğŸ”§ TETRA Cache Optimizasyon Yol HaritasÄ±

> **Son GÃ¼ncelleme:** AralÄ±k 2024  
> **Durum:** FAZ 1 TamamlandÄ±, FAZ 2 Beklemede

---

## ğŸ“Š Genel BakÄ±ÅŸ

TETRA mÃ¼nazara platformu iÃ§in iki aÅŸamalÄ± cache stratejisi planlandÄ±:

| FAZ | AÃ§Ä±klama | Durum | Dosya |
|-----|----------|-------|-------|
| FAZ 1 | Provider-Based Prompt Caching | âœ… **TAMAMLANDI** | `openRouterService.ts` |
| FAZ 2 | Backend Response Cache | â¸ï¸ **BEKLEMEDE** | - |

---

## âœ… FAZ 1: Provider-Based Prompt Caching (TAMAMLANDI)

### Ne YapÄ±yor?
- OpenRouter API'ye gÃ¶nderilen system prompt'lar iÃ§in provider'a Ã¶zel cache formatÄ± kullanÄ±yor
- Anthropic/Google: `cache_control: { type: 'ephemeral' }` flag'i
- OpenAI/DeepSeek/Grok: Otomatik prefix caching (doÄŸru sÄ±ralama yeterli)

### Beklenen Tasarruf
| Provider | Cache Read Tasarrufu |
|----------|---------------------|
| Anthropic (Claude) | %90 |
| DeepSeek | %90 |
| OpenAI (GPT-4, o1) | %50-75 |
| Google (Gemini) | %75 |
| Grok | %75 |

### Dosya
`app/src/services/openRouterService.ts`

---

## â¸ï¸ FAZ 2: Backend Response Cache (BEKLEMEDE)

### Ne Yapacak?
AynÄ± istek (aynÄ± model + aynÄ± mesajlar) tekrar gelirse, API'ye hiÃ§ gitmeden backend cache'den dÃ¶necek.

### Ne Zaman Gerekli?
- âŒ Normal mÃ¼nazara ilerlemesi â†’ Her tur farklÄ± baÄŸlam, cache HIT etmez
- âœ… Hata sonrasÄ± retry â†’ AynÄ± istek tekrar gÃ¶nderilir
- âœ… Interrupt/kesinti sonrasÄ± devam â†’ AynÄ± yerden devam
- âš ï¸ Sayfa yenileme â†’ State zaten kaybolmuÅŸ olabilir

### Neden Åimdi YapmadÄ±k?
MÃ¼nazarada baÄŸlam sÃ¼rekli evrimleÅŸiyor:
```
TUR 1: [System] + [Mesaj1]                    â†’ Hash A
TUR 2: [System] + [Mesaj1] + [Mesaj2]         â†’ Hash B (farklÄ±!)
TUR 3: [System] + [Mesaj1] + [Mesaj2] + [M3]  â†’ Hash C (farklÄ±!)
```
Her tur farklÄ± hash Ã¼retir â†’ Cache neredeyse hiÃ§ HIT etmez.

### Implementasyon PlanÄ± (Gerekirse)

#### 1. Backend Endpoint'i
```javascript
// server.js'e eklenecek

// --- RESPONSE CACHE ---
app.get('/api/cache/:hash', (req, res) => {
  const { hash } = req.params;
  const cache = readData('response-cache.json') || {};
  
  if (cache[hash] && Date.now() - cache[hash].timestamp < 24 * 60 * 60 * 1000) {
    console.log(`[CACHE HIT] Hash: ${hash.substring(0, 8)}...`);
    return res.json({ hit: true, data: cache[hash] });
  }
  
  res.json({ hit: false });
});

app.post('/api/cache/:hash', (req, res) => {
  const { hash } = req.params;
  const { response, modelId, tokens } = req.body;
  const cache = readData('response-cache.json') || {};
  
  cache[hash] = {
    response,
    modelId,
    tokens,
    timestamp: Date.now()
  };
  
  // Cache boyutunu kontrol et (max 1000 entry)
  const keys = Object.keys(cache);
  if (keys.length > 1000) {
    // En eski 100 entry'yi sil
    const sortedKeys = keys.sort((a, b) => cache[a].timestamp - cache[b].timestamp);
    sortedKeys.slice(0, 100).forEach(k => delete cache[k]);
  }
  
  writeData('response-cache.json', cache);
  res.json({ success: true });
});
```

#### 2. Frontend Cache Utility
```typescript
// cacheUtils.ts - OluÅŸturulacak

// SHA-256 hash fonksiyonu
async function generateCacheKey(modelId: string, messages: any[]): Promise<string> {
  const encoder = new TextEncoder();
  const data = JSON.stringify({ modelId, messages });
  const hashBuffer = await crypto.subtle.digest('SHA-256', encoder.encode(data));
  const hashArray = Array.from(new Uint8Array(hashBuffer));
  return hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
}

export const ResponseCache = {
  async get(modelId: string, messages: any[]): Promise<any | null> {
    const hash = await generateCacheKey(modelId, messages);
    const res = await fetch(`/api/cache/${hash}`);
    const data = await res.json();
    return data.hit ? data.data : null;
  },

  async set(modelId: string, messages: any[], response: any, tokens: number): Promise<void> {
    const hash = await generateCacheKey(modelId, messages);
    await fetch(`/api/cache/${hash}`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ response, modelId, tokens })
    });
  }
};
```

#### 3. openRouterService.ts Entegrasyonu
```typescript
// sendChatCompletion fonksiyonunun baÅŸÄ±na eklenecek

// Cache kontrolÃ¼ (FAZ 2)
const cachedResponse = await ResponseCache.get(modelId, messages);
if (cachedResponse) {
  console.log(`[CACHE] ğŸ¯ Backend cache HIT! API'ye gidilmedi.`);
  return {
    text: cachedResponse.response,
    usage: cachedResponse.tokens,
    cost: 0, // Cache'den geldi, maliyet yok
    cachedTokens: cachedResponse.tokens,
    fromCache: true
  };
}

// ... normal API Ã§aÄŸrÄ±sÄ± ...

// BaÅŸarÄ±lÄ± response'u cache'e yaz
await ResponseCache.set(modelId, messages, result.text, result.usage);
```

### Bu DosyayÄ± Ne Zaman GÃ¼ncellemeliyiz?
- [ ] Retry oranÄ± %5'i geÃ§erse
- [ ] KullanÄ±cÄ±lardan "aynÄ± yanÄ±tÄ± tekrar aldÄ±m" ÅŸikayetleri gelirse
- [ ] Maliyet analizi yapÄ±ldÄ±ÄŸÄ±nda gereksiz API Ã§aÄŸrÄ±larÄ± tespit edilirse

---

## ğŸ“ˆ Maliyet Takibi (Ä°leride)

FAZ 1 ve FAZ 2'nin etkisini Ã¶lÃ§mek iÃ§in:

```typescript
// Telemetri verisi
interface CacheStats {
  totalRequests: number;
  promptCacheHits: number;      // FAZ 1 - Provider cache
  responseCacheHits: number;    // FAZ 2 - Backend cache
  estimatedSavings: number;     // USD
}
```

---

## ğŸ”— Ä°lgili Dosyalar

- `app/src/services/openRouterService.ts` - FAZ 1 implementasyonu
- `backend/server.js` - Backend API
- `backend/data/response-cache.json` - FAZ 2 cache dosyasÄ± (oluÅŸturulacak)

---

## ğŸ“ Notlar

> **Karar (AralÄ±k 2024):** FAZ 2 ÅŸimdilik atlandÄ± Ã§Ã¼nkÃ¼ mÃ¼nazara senaryosunda her tur farklÄ± baÄŸlam Ã¼retiyor. Cache hit oranÄ± Ã§ok dÃ¼ÅŸÃ¼k olacaÄŸÄ± iÃ§in gereksiz karmaÅŸÄ±klÄ±k eklememek tercih edildi. Hata/interrupt senaryolarÄ± iÃ§in gerekirse ileride eklenecek.



